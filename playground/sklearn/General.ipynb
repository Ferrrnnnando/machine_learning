{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66283346",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b720b0f",
   "metadata": {},
   "source": [
    "## Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09673ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6198177",
   "metadata": {},
   "source": [
    "## Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88bc6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/107058797\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224b38f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc743d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Data scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb7c90",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976280b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.feature_selection import mutual_info_regression # for real-valued targets \n",
    "from sklearn.feature_selection import mutual_info_classif # for categorical targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f988a9f",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Boost\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ef3a6",
   "metadata": {},
   "source": [
    "## Tunning Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd17c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb993ec9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaca789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f487c",
   "metadata": {},
   "source": [
    "## Saving Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e57534",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11564ddd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"end_to_end_project\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240cb69",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec58005",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Fetch from Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5ee53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d7075",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41279f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "data = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012c2ff",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data obeservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b28d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "housing.head()\n",
    "housing.info()\n",
    "housing.describe()\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69492d52",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(20,15))\n",
    "save_fig(\"attribute_histogram_plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97f963",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725e261",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5742b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pyplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e3665",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)# deeper color for dense area\n",
    "save_fig(\"bad_visualization_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02d9bc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Line Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.title(\"Daily Global Streams of Popular Songs in 2017-2018\")\n",
    "sns.lineplot(data=spotify_data)\n",
    "\n",
    "# Plot 2 curves on the same plot\n",
    "# Line chart showing daily global streams of 'Shape of You'\n",
    "sns.lineplot(data=spotify_data['Shape of You'], label=\"Shape of You\")\n",
    "# Line chart showing daily global streams of 'Despacito'\n",
    "sns.lineplot(data=spotify_data['Despacito'], label=\"Despacito\")\n",
    "# Add label for horizontal axis\n",
    "plt.xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5743c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfcf844",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Bar chart showing average arrival delay for Spirit Airlines flights by month\n",
    "sns.barplot(x=flight_data.index, y=flight_data['NK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bebd52",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05da61d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Heatmap showing average arrival delay for each airline by month\n",
    "sns.heatmap(data=flight_data, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a2712",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50a1ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Without regression line\n",
    "sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'])\n",
    "\n",
    "sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'], hue=insurance_data['smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad86fc8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# With regression line\n",
    "sns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])\n",
    "\n",
    "sns.lmplot(x=\"bmi\", y=\"charges\", hue=\"smoker\", data=insurance_data)# 2 reg line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a7fcf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef82a51",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Histogram \n",
    "sns.distplot(a=iris_data['Petal Length (cm)'], kde=False)\n",
    "\n",
    "# KDE plot (\"smooth histogram\")\n",
    "sns.kdeplot(data=iris_data['Petal Length (cm)'], shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7adde2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2D KDE plots\n",
    "sns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa0dae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3 histograms on same graph\n",
    "\n",
    "# Histograms for each species\n",
    "sns.distplot(a=iris_set_data['Petal Length (cm)'], label=\"Iris-setosa\", kde=False)\n",
    "sns.distplot(a=iris_ver_data['Petal Length (cm)'], label=\"Iris-versicolor\", kde=False)\n",
    "sns.distplot(a=iris_vir_data['Petal Length (cm)'], label=\"Iris-virginica\", kde=False)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Histogram of Petal Lengths, by Species\")\n",
    "\n",
    "# Force legend to appear\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70242988",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "housing.info()\n",
    "housing[\"ocean_proximity\"].value_counts()\n",
    "housing.describe()# return statistical result of each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ad0d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Choose suitable predictors(columns)\n",
    "\n",
    "s = (X_train.dtypes == 'object')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Or:\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "## Final remainging columns\n",
    "my_cols = low_cardinality_cols + numerical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1616d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23efacb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9181e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Method 2: Use pandas default method\n",
    "df_train = customer.sample(frac=0.5)\n",
    "df_valid = customer.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a47192",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Divide income into different categories (5 ranges)\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Samples for each catagories will have roughly same proportion in both train and test sets\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# check the proportion strat vs. original\n",
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af647394",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc9062",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c697768",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb6f132",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ebfff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f1c6c4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f653cfd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547368e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "# final_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Only fit using train data, and apply fitted data to valid data(new)\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "# or: \n",
    "# housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80956fab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "myimputer.statistics_\n",
    "# array([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,\n",
    "#         408.    ,    3.5409])\n",
    "imputer.strategy # median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484a3f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Method 2:\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c451ebc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Extension To Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0de9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Next, we impute the missing values, while also keeping track of which values were imputed.\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621be345",
   "metadata": {},
   "source": [
    "## Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a1f3e",
   "metadata": {},
   "source": [
    "### Drop Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1174fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The easiest approach to dealing with categorical variables is to simply remove them from the dataset. \n",
    "# This approach will only work well if the columns did not contain useful information.\n",
    "\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93296530",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2458f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09922ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_ \n",
    "# [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71b3d9",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f07d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4355021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params:\n",
    "# 1. handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data\n",
    "# 2. setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method:\n",
    "housing_cat_1hot.toarray()\n",
    "'''\n",
    "array([[1., 0., 0., 0., 0.],\n",
    "       [1., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 1.],\n",
    "       ...\n",
    "'''\n",
    "\n",
    "# Method 2:\n",
    "# Alternatively, you can set sparse=False when creating the OneHotEncoder:\n",
    "cat_encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb2fbf1",
   "metadata": {},
   "source": [
    "### General Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categoricals\n",
    "# Encoding each distinct catogory from 0, 1, 2, ...\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563df352",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70b852",
   "metadata": {},
   "source": [
    "### From Website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!! Example 1 Standard Way!!!!!\n",
    "\n",
    "# Step1: Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Step 2: Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# Step 3: Bundle preprocessing for both types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 4: Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(\n",
    "    steps=[('preprocessor', SimpleImputer()),\n",
    "           ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n",
    "          ]\n",
    ")\n",
    "\n",
    "# Step 5: Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)\n",
    "\n",
    "\n",
    "\n",
    "# !!!!! Example 2 : Use of make_pipeline() func!!!!!\n",
    "\n",
    "my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\n",
    "cv_scores = cross_val_score(my_pipeline, X, y, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7688d",
   "metadata": {},
   "source": [
    "### From Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ac7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1 : Numerical pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# For single numerical pipeline, do both 'fit' and 'transform'\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a category transformer and combine it with numerical one to form a single transformer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6827dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra: Create a sample result by using a different solution:\n",
    "# Use of an old solution called 'OldDataFrameSelector' instead of 'ColumnTransformer'\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # Must use when creating transfomer\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# Usage of this transfomer: To just select a subset of the Pandas DataFrame columns\n",
    "class OldDataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Put selector into each pipeline to select specific columns\n",
    "old_num_pipeline = Pipeline([\n",
    "        ('selector', OldDataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "old_cat_pipeline = Pipeline([\n",
    "        ('selector', OldDataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False)),\n",
    "])\n",
    "\n",
    "# Form a single pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "old_full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", old_num_pipeline),\n",
    "        (\"cat_pipeline\", old_cat_pipeline),\n",
    "])\n",
    "\n",
    "old_housing_prepared = old_full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b1f83",
   "metadata": {},
   "source": [
    "#### Pipeline with both preparation and prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really full pipeline: Preparation & Selection & Prediction\n",
    "\n",
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('svm_reg', SVR(**rnd_search.best_params_))\n",
    "])\n",
    "\n",
    "# Just use 'fit' if prediction is included\n",
    "prepare_select_and_predict_pipeline.fit(housing, housing_labels)\n",
    "prepare_select_and_predict_pipeline.predict(some_data)\n",
    "\n",
    "'''\n",
    "Pipeline(steps=[('preparation',\n",
    "                 ColumnTransformer(transformers=[('num',\n",
    "                                                  Pipeline(steps=[('imputer',\n",
    "                                                                   SimpleImputer(strategy='median')),\n",
    "                                                                  ('attribs_adder',\n",
    "                                                                   CombinedAttributesAdder()),\n",
    "                                                                  ('std_scaler',\n",
    "                                                                   StandardScaler())]),\n",
    "                                                  ['longitude', 'latitude',\n",
    "                                                   'housing_median_age',\n",
    "                                                   'total_rooms',\n",
    "                                                   'total_bedrooms',\n",
    "                                                   'population', 'households',\n",
    "                                                   'median_income']),\n",
    "                                                 ('cat', OneHotEncoder(...\n",
    "                 TopFeatureSelector(feature_importances=array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\n",
    "       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\n",
    "       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n",
    "       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]),\n",
    "                                    k=5)),\n",
    "                ('svm_reg',\n",
    "                 SVR(C=157055.10989448498, gamma=0.26497040005002437))])\n",
    "'''\n",
    "# Treat the full pipeline as a single model\n",
    "my_model = prepare_select_and_predict_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Make pipeline component from scratch\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def indices_of_top_k(arr, k):\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]\n",
    "    \n",
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])\n",
    "\n",
    "# call fit_transform method on new data\n",
    "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35adf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])\n",
    "\n",
    "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Pipeline for all\n",
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('svm_reg', SVR(**rnd_search.best_params_))\n",
    "])\n",
    "\n",
    "# Fit only\n",
    "prepare_select_and_predict_pipeline.fit(housing, housing_labels)\n",
    "# Predict\n",
    "prepare_select_and_predict_pipeline.predict(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbbe81",
   "metadata": {},
   "source": [
    "## Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce4315",
   "metadata": {},
   "source": [
    "## Scale and Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Scale the data to within range 0-1\n",
    "scaled_data = minmax_scaling(original_data, columns=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both together to compare\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
    "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a908ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Standardize: change the shape (to Normal Distribution)\n",
    "\n",
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# OR:\n",
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fcdc21",
   "metadata": {},
   "source": [
    "## Parsing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cf6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of our date column: cols storing date are usually 'Object' type\n",
    "landslides['date'].dtype\n",
    "\n",
    "# Result: dtype('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column, date_parsed, with the parsed dates\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first few rows\n",
    "landslides['date_parsed'].head()\n",
    "'''\n",
    "0   2007-03-02\n",
    "1   2007-03-22\n",
    "2   2007-04-06\n",
    "3   2007-04-14\n",
    "4   2007-04-15\n",
    "Name: date_parsed, dtype: datetime64[ns]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251734b7",
   "metadata": {},
   "source": [
    "## Character Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# try to read in a file not in UTF-8, default encoding system is UTF-8, will have error\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: check some part of file to determine what type of encoding is using\n",
    "\n",
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e340294",
   "metadata": {},
   "source": [
    "## Inconsistent Data Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "professors['Country'] = professors['Country'].str.lower()\n",
    "# remove trailing white spaces\n",
    "professors['Country'] = professors['Country'].str.strip()\n",
    "\n",
    "countries = professors['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18667c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 closest matches to \"south korea\"\n",
    "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "'''\n",
    "[('south korea', 100),\n",
    " ('southkorea', 48),\n",
    " ('saudi arabia', 43),\n",
    " ...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")\n",
    "\n",
    "    \n",
    "# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\n",
    "replace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f79c93",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe06fb",
   "metadata": {},
   "source": [
    "## Correlations (linear) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "\n",
    "# Find the correlation of each feature with the median house value\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12296a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x4 correlation matrix, find the mutual correlation between these 4 features\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "save_fig(\"scatter_matrix_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af329275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one specific scatter graph between one feature and the another\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])\n",
    "save_fig(\"income_vs_house_value_scatterplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b7487",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e6864",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Mutual information is a lot like correlation in that it measures a relationship between \n",
    "two quantities. The advantage of mutual information is that it can detect any kind of relationship, \n",
    "while correlation only detects linear relationships.\n",
    "\n",
    "Mutual information describes relationships in terms of uncertainty. \n",
    "The mutual information (MI) between two quantities is a measure of the extent to \n",
    "which knowledge of one quantity reduces uncertainty about the other. \n",
    "\n",
    "When MI is zero, the quantities are independent: neither can tell you anything about the other.\n",
    "In practice though values above 2.0 or so are uncommon.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a01f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Find the MI between the target and each features\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    # Only plug in discrete features\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    # Make a pandas series to store the result\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores\n",
    "\n",
    "'''\n",
    "curb_weight          1.477695\n",
    "highway_mpg          0.953932\n",
    "length               0.621122\n",
    "fuel_system          0.484737\n",
    "stroke               0.383782\n",
    "num_of_cylinders     0.330589\n",
    "compression_ratio    0.133822\n",
    "fuel_type            0.048139\n",
    "Name: MI Scores, dtype: float64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0294097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot MI graphs for all features wrt target:\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "    \n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb146ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot individual relationship between a feature and the target\n",
    "# Relationship between \"curbweight\" and target \"price\"\n",
    "\n",
    "sns.relplot(x=\"curb_weight\", y=\"price\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0bc7c0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Relationship between the target and horsepower under different types of fuel\n",
    "# there will be two regression line, each for a fuel type\n",
    "\n",
    "sns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd246792",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b2eb1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "autos[\"stroke_ratio\"] = autos.stroke / autos.bore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daede323",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data visualization can suggest transformations, often a \"reshaping\" of a feature through powers or logarithms.\n",
    "\n",
    "# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\n",
    "accidents[\"LogWindSpeed\"] = accidents.WindSpeed.apply(np.log1p)\n",
    "\n",
    "# Plot a comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])\n",
    "sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d9788",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sum across columns to find the sub sum for each row\n",
    "\n",
    "# For true/false columns\n",
    "roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n",
    "    \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "    \"TrafficCalming\", \"TrafficSignal\"]\n",
    "accidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n",
    "\n",
    "# For numerical columns: sum up across rows if each item is greater than 0\n",
    "concrete[\"Components\"] = concrete[components].gt(0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a34c74",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Split one string feature into 2\n",
    "\n",
    "customer[[\"Type\", \"Level\"]] = (  # Create two new features\n",
    "    customer[\"Policy\"]           # from the Policy feature\n",
    "    .str                         # through the string accessor\n",
    "    .split(\" \", expand=True)     # by splitting on \" \"\n",
    "                                 # and expanding the result into separate columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a62cbc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Group Transforms: each element within the same group will be given the same value\n",
    "\n",
    "customer[\"AverageIncome\"] = (\n",
    "    customer.groupby(\"State\")  # for each state\n",
    "    [\"Income\"]                 # select the income\n",
    "    .transform(\"mean\")         # and compute its mean, max, min, median, var, std, and count\n",
    ")\n",
    "\n",
    "# Example 2:\n",
    "df_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b036f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge the values into the validation set\n",
    "\n",
    "df_valid = df_valid.merge(\n",
    "    df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(), # drop duplicate rows\n",
    "    on=\"Coverage\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e6d799",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clustering with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae08ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Since k-means clustering is sensitive to scale, it can be a good idea \n",
    "# rescale or normalize data with extreme values.\n",
    "\n",
    "# Create cluster feature\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab8b96",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Draw K-means graph\n",
    "sns.relplot(\n",
    "    x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d9a30",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Draw a box diagram to show the distribution of each clusters\n",
    "# If the clustering is informative, these distributions should, \n",
    "# for the most part, separate across MedHouseVal\n",
    "\n",
    "X[\"MedHouseVal\"] = df[\"MedHouseVal\"]\n",
    "sns.catplot(x=\"MedHouseVal\", y=\"Cluster\", data=X, kind=\"boxen\", height=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2745ff",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa11ac",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bd58a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PCA \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create principal components\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfde411",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50befd2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1. Mean encoding\n",
    "autos[\"make_encoded\"] = autos.groupby(\"make\")[\"price\"].transform(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d4ca8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2. Smoothing: deal with rare & unknown categories\n",
    "\n",
    "# Pseudocode: encoding = weight * in_category + (1 - weight) * overall\n",
    "# m-estimate: weight = n / (n + m)\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Create the encoder instance. Choose m to control noise.\n",
    "encoder = MEstimateEncoder(cols=[\"Zipcode\"], m=5.0)\n",
    "\n",
    "# Fit the encoder on the encoding split.\n",
    "encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# Encode the Zipcode column to create the final training data\n",
    "X_train = encoder.transform(X_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e01b3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare the encoded values to the target to see how informative our encoding might be.\n",
    "\n",
    "plt.figure(dpi=90)\n",
    "ax = sns.distplot(y, kde=False, norm_hist=True)\n",
    "ax = sns.kdeplot(X_train.Zipcode, color='r', ax=ax)\n",
    "ax.set_xlabel(\"Rating\")\n",
    "ax.legend(labels=['Zipcode', 'Rating']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15f532",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac53582",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cbbca",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbf98a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113eb6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0783e2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Custom Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebeda8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4301a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29337376",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35dc7c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Idea:\n",
    "# Gradient boosting is a ensemble method other than random forest \n",
    "# that goes through cycles to iteratively add models into an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8b0b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)],\n",
    "             verbose=False)\n",
    "\n",
    "'''\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "             gamma=0, gpu_id=-1, importance_type=None,\n",
    "             interaction_constraints='', learning_rate=0.300000012,\n",
    "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
    "             monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
    "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
    "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
    "             validate_parameters=1, verbosity=None)\n",
    "'''\n",
    "\n",
    "'''\n",
    "1. n_estimators: specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n",
    "Typical values range from 100-1000\n",
    "2. early_stopping_rounds: offers a way to automatically find the ideal value for n_estimators. \n",
    "Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. \n",
    "3. learning_rate: we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in.\n",
    "4. n_jobs: equal to the number of cores on your machine.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fc2a6",
   "metadata": {},
   "source": [
    "#  Tuning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275e213",
   "metadata": {},
   "source": [
    "## Normal Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_ # {'max_features': 8, 'n_estimators': 30}\n",
    "grid_search.best_estimator_ # RandomForestRegressor(max_features=8, n_estimators=30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score of each hyperparameter combination tested during the grid search:\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "    \n",
    "'''\n",
    "There will be 12 + 6 = 18 results\n",
    "63669.11631261028 {'max_features': 2, 'n_estimators': 3}\n",
    "55627.099719926795 {'max_features': 2, 'n_estimators': 10}\n",
    "53384.57275149205 {'max_features': 2, 'n_estimators': 30}\n",
    "60965.950449450494 {'max_features': 4, 'n_estimators': 3}\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SVM regresssor\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "svm_reg = SVR()\n",
    "grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "grid_search.best_params_ # {'C': 30000.0, 'kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99e47d",
   "metadata": {},
   "source": [
    "## Randomized Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0044fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Random Forest Regressor\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "# Randomly pick values for n_estimators and max_features from the required range\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "'''\n",
    "n_iter=10 : run 10 times\n",
    "49150.70756927707 {'max_features': 7, 'n_estimators': 180}\n",
    "51389.889203389284 {'max_features': 5, 'n_estimators': 15}\n",
    "50796.155224308866 {'max_features': 3, 'n_estimators': 72}\n",
    "50835.13360315349 {'max_features': 5, 'n_estimators': 21}\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9989e",
   "metadata": {},
   "source": [
    "## Feature Importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "'''\n",
    "array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\n",
    "       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\n",
    "       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n",
    "       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5680a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(feature_importances, attributes), reverse=True)\n",
    "'''\n",
    "[(0.36615898061813423, 'median_income'),\n",
    " (0.16478099356159054, 'INLAND'),\n",
    " (0.10879295677551575, 'pop_per_hhold'),\n",
    " ...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f9215",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095b60c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e199e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data using imputers\n",
    "final_X_test = pd.DataFrame(final_imputer.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147ad6d",
   "metadata": {},
   "source": [
    "## Best Model from Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697e969",
   "metadata": {},
   "source": [
    "## Simple Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(final_X_train, y_train)\n",
    "\n",
    "# Get validation predictions and MAE\n",
    "preds_valid = model.predict(final_X_valid)\n",
    "print(\"MAE (Your approach):\")\n",
    "print(mean_absolute_error(y_valid, preds_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965801ab",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05331ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)\n",
    "\n",
    "print(\"Average MAE score (across experiments):\")\n",
    "print(scores.mean())\n",
    "\n",
    "# MAE scores:\n",
    "#  [301628.7893587  303164.4782723  287298.331666   236061.84754543\n",
    "#  260383.45111427]\n",
    "\n",
    "# Average MAE score (across experiments):\n",
    "# 277707.3795913405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9828c5d4",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(my_model, \"my_model.pkl\")\n",
    "my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc228c2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd391a1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd236711",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X[\"Cluster\"].astype(\"category\")# change to another type\n",
    "countries = professors['Country'].unique()\n",
    "countries.sort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.19021606445312px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
