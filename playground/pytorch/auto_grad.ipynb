{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"auto_grad.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdKjUTo2M5ZUbJlZGxqYhb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"QXSwarQPtqeD","executionInfo":{"status":"ok","timestamp":1644750322990,"user_tz":-480,"elapsed":6104,"user":{"displayName":"Fernando CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06486171134930786741"}}},"outputs":[],"source":["import torch"]},{"cell_type":"code","source":["# w and b are parameters that we need to optimize\n","# Compute the gradient of the loss function with respect to those variables\n","# so we set requiress_grad param to be true\n","\n","x = torch.ones(5)   # input tensor\n","y = torch.zeros(3)  # expected output\n","\n","w = torch.randn(5, 3, requires_grad=True)\n","print('w: ', w)\n","b = torch.randn(3, requires_grad=True)\n","print('b: ', b)\n","\n","z = torch.matmul(x, w) + b\n","print('z: ', z)\n","\n","# We compare and calculated the difference between predicted results: z \n","# and the expected result by using cross entropy\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3K-M-_OujMI","executionInfo":{"status":"ok","timestamp":1644751021988,"user_tz":-480,"elapsed":327,"user":{"displayName":"Fernando CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06486171134930786741"}},"outputId":"fa5040c1-920f-4d84-a2c0-d9ca1903c3c5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["w:  tensor([[ 1.0671,  0.4779,  1.5446],\n","        [ 0.0640, -0.6595, -0.2024],\n","        [-0.5631, -0.3336, -1.0392],\n","        [-0.6145,  1.1067,  2.3310],\n","        [-1.2196, -1.6501,  0.7928]], requires_grad=True)\n","b:  tensor([-0.4092,  0.9760, -0.6199], requires_grad=True)\n","z:  tensor([-1.6753, -0.0827,  2.8069], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["torch.matmul(x, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dkpRKR_vaKy","executionInfo":{"status":"ok","timestamp":1644750441415,"user_tz":-480,"elapsed":8,"user":{"displayName":"Fernando CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06486171134930786741"}},"outputId":"a853a864-fd02-466b-b422-c10432dca820"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJZr_YRoxdHr","executionInfo":{"status":"ok","timestamp":1644752115030,"user_tz":-480,"elapsed":995,"user":{"displayName":"Fernando CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06486171134930786741"}},"outputId":"26b73d7f-42c3-4c5e-9b7e-995a7549d335"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.2299, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"]}]},{"cell_type":"code","source":["print('Gradient fucntion for z = ', z.grad_fn)\n","\n","# A reference to the backward propagation function is stored in grad_fn property of a tensor\n","print('Gradient function for loss = ', loss.grad_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-FlVh9uxAcH","executionInfo":{"status":"ok","timestamp":1644751257863,"user_tz":-480,"elapsed":282,"user":{"displayName":"Fernando CHEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06486171134930786741"}},"outputId":"a3ed7fb2-c701-4bdf-c065-37c83be3341c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient fucntion for z =  <AddBackward0 object at 0x7f43a9e0db10>\n","Gradient function for loss =  <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f43a9e0d090>\n"]}]},{"cell_type":"code","source":["# Compute derivatives of loss function by backward() with respect to w and b\n","loss.backward()\n","\n","# Retrieve the values from w.grad and b.grad\n","print(w.grad)\n","print(b.grad)"],"metadata":{"id":"W5xahVe6xQVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Notes:\n","By default, all tensors with requires_grad=True\n","are tracking their computational history and \n","support gradient computation.\n","'''"],"metadata":{"id":"OvLgUMEwxzMY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Disable Gradient Tracking"],"metadata":{"id":"9AgOIbsU1Lj2"}},{"cell_type":"code","source":["# Stop gradient tracking computation by torch.no_grad\n","\n","z = torch.matmul(x, w) + b\n","print(z.requires_grad)\n","\n","# Method 1\n","with torch.no_grad():\n","    z = torch.matmul(x, w) + b\n","print(z.requires_grad)\n","\n","# Method 2\n","z_det = z.detach()\n","\n","# Result:\n","# True\n","# False"],"metadata":{"id":"NsfmjuDs1Ojq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Reasons you might want to disable gradient tracking:\n","1. To mark some parameters in your neural network as frozen parameters. This is a very common scenario for finetuning a pretrained network\n","2. To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n","'''"],"metadata":{"id":"jkaAaVI_2B6Y"},"execution_count":null,"outputs":[]}]}